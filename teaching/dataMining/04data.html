<!doctype html>
<html lang="zh-Hant-TW">

<head>
<meta charset="utf-8">

<title>資料 (Data)</title>

<meta name="description" content="Tutorial of web design for primer">
<meta name="author" content="Elvis Hsieh">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="/revealjs/dist/reset.css">
<link rel="stylesheet" href="/revealjs/dist/reveal.css">
<link rel="stylesheet" href="/revealjs/dist/theme/black.css" id="theme">
<link rel="stylesheet" href="/revealjs/plugin/chalkboard/style.css">
<link rel="stylesheet" href="/revealjs/plugin/customcontrols/style.css">
<link rel="stylesheet" href="/css/custom4revealjs.css">
<!-- Code syntax highlighting -->
<link rel="stylesheet" href="/highlightjs/styles/monokai.min.css">   
<link rel="stylesheet" href="/css/style4font.css">
<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /revealjs/print-pdf/gi ) ? '/revealjs/dist/print/pdf.css' : '/revealjs/dist/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<!--[if lt IE 9]>
<script src="reveal-js/lib/js/html5shiv.js"></script>
<![endif]-->
</head>
<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section><!--start block-->
<section data-markdown><script type="text/template">
# Data Objects <br />and <br />Attribute Types
</script></section>
<section data-markdown><script type="text/template">
## Data Objects
- Data sets are **made up** (組成) of data objects. A **data object** represents an entity.
	- In a university database, the objects may be students, professors and courses.
	- In a sales database, the objects may be customers, store items and sales.
- Data objects can also be referred to as **samples, examples, instances, data points or objects**.
-  If the data objects are stored in a database, they are data tuples. That is, the **rows** of a database correspond to the **data objects**, and the **columns** correspond to the **attributes**. 
</script></section>
<section data-markdown><script type="text/template">
## What Is an Attribute?
- An attribute is a data field, representing a characteristic or feature of a data object.
- The nouns **attribute, dimension, feature, and variable** are often used interchangeably in the literature.
	- The term dimension is commonly used in **data warehousing**.
	- **Machine learning** literature tends to use the term feature, while **statisticians** prefer the term variable.
	- **Data mining and database** professionals commonly use the term attribute
</script></section>
<section data-markdown><script type="text/template">
## List of feature and instance
![](/images/datamining/attribute.png)
</script></section>
<section data-markdown><script type="text/template">
## Attribute Types(1/2)
- **Nominal** Attributes
	- Nominal means “relating to names.” The values of a nominal attribute are **symbols or names of things**.
	- Hair_color = **{black, blond, brown, grey, red, white}**
	- marital status, occupation, ID numbers, zip codes
- **Binary** Attributes
	- A binary attribute is a nominal attribute with only two categories or states: **0 or 1**,
	- where	**0** typically means that the attribute is **absent**, and **1** means that it is **present**.
</script></section>
<section data-markdown><script type="text/template">
## Attribute Types(2/2)
- **Ordinal** Attributes
	- Values have a **meaningful order (ranking)** but magnitude between successive values is not known.
	- Size = **{small, medium, large}**, grades, army rankings
- **Numeric** Attributes
	- A numeric attribute is **quantitative**; that is, it is a measurable quantity, represented in	**integer or real values.**
	- Numeric attributes can be **interval-scaled or ratio-scaled**
</script></section>
<section data-markdown><script type="text/template">
## Numeric Attribute Types
- Interval-scaled
	- Measured on a scale of **equal-sized units**
	- Values have order
		- E.g., temperature in C˚or F˚, calendar dates
	- **No true zero-point**
- Ratio-scaled
	- **Inherent(固有的) zero-point**
	- We can speak of values as being an order of magnitude larger than the unit of measurement (10 K˚ is twice as high as 5 K˚).
		- e.g., temperature in Kelvin, length, counts, monetary quantities
</script></section>
<section data-markdown><script type="text/template">
## Summary of the Characteristics for Levels of Measurement
![](/images/datamining/measurement.png)
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Discrete attribute<br /> vs. <br />Continuous attribute
</script></section>
<section data-markdown><script type="text/template">
## Discrete vs. Continuous
- A **discrete** attribute has a **finite or countably infinite** set of values, which may or may not be represented as integers.
	- Hair_color, ID numbers, zip codes, integer variables
	- Binary are a special case of discrete attributes 
- If an attribute is **not discrete, it is continuous**. The terms **numeric** and **continuous** are often used interchangeably in the literature.
	- This can be confusing	because, **continuous values are real numbers**, whereas **numeric values** can be either **integers** or **real numbers**.
	- Continuous attributes are typically represented as **floating-point** variables.
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Basic Statistical Descriptions of Data
</script></section>
<section data-markdown><script type="text/template">
## Measuring the Central Tendency
- Suppose that we have some attribute `$X$`, like salary, which has been recorded for a set of objects.
Let `$ x_1, x_2, \cdots , x_N $` be the set of `$N$` observed values or observations for `$X$`. The **mean** of this set of values is
`$$ \overline{x}=\frac{1}{N} \sum_{i=1}^{N} x_{i}=\frac{x_{1}+\cdots+x_{N}}{N} $$`
- Example 2.6 Mean. Suppose we have the following values for salary (in thousands of dollars), shown
in increasing order: 30, 36, 47, 50, 52, **52, 56**, 60, 63, 70, 70, **110**. We have
`$\overline{x}=\frac{30+36+47+50+52+52+56+60+63+70+70+110}{12} = 58$`
</script></section>
<section data-markdown><script type="text/template">
## Mean, Median, and Mode (1/4)
- **Weighted** arithmetic mean: 
`$$ \overline{x}=\frac{\sum_{i=1}^{N} w_{i} \cdot x_{i}} {\sum_{i=1}^{N} w_{i}} = \frac{w_{1} x_{1}+ w_{2} x_{2}+\cdots + w_{N} x_{N}}{w_{1}+ w_{2}+\cdots + w_{N}}$$`
- Median
	- **Middle value** if **odd** number of values, or **average** of the **middle two values** otherwise
	- Let's find the median of the data **from Example 2.6**. That is `$\frac{52+56}{2} = \frac{108}{2} = 54$`
	- Suppose that we had only the first 11 values in the list. Given an odd number of values, the median **(52)** is the middlemost value.
</script>
</section>
<section data-markdown><script type="text/template">
## Mean, Median, and Mode (2/4)
- Median
	- Estimated by interpolation (for grouped data):
`$$ median = L_{1} + \left( \frac{N/2 - (\sum freq)_l}{freq_{median}} \right) \times width$$`

	- where `$L_{1}$` is the **lower boundary** of the group containing the median, `$N$` is the number of values in the 
	entire data set,`$(\sum freq)_{l}$` is the **sum of the frequencies** of the groups **before the median group**, `$freq_{median}$` is the 
	frequency of the median group, and `$width$` is the **width of the median group**.
</script>
</section>
<section data-markdown><script type="text/template">
## Example of Mdeian (1/2)
- Suppose that the values for a given set of data are grouped into intervals. The intervals and corresponding
frequencies are as follows. Compute an **approximate median value** for the data.
<figure style="float: right">
<img src="/images/datamining/median.png" alt="median example">
</figure>
- Answer
	- `$N = 200+450+300+1500+700+44 = 3194$`
	- `$N /2 = 1597$`, median group is <br />**<`$age$`:21-50 `$frequency$`: 1500>**, and lower boundary of median group `$L$` is 20
	- `$(\sum freq)_{l}$ = 200+450+300 = 950`
</script>
</section>
<section data-markdown><script type="text/template">
## Example of Mdeian (2/2)
- Answer

	- `$freq_{median}$ = 1500`
	- **`$width$ = 50-20 = 30`**
	 `$$ \begin{align*}
		median & = L_{1} + \left( \frac{N/2 - (\sum freq)_l}{freq_{median}} \right) \times width \\
		 	   & = 20 + \left( \frac{3194/2 - (950)}{1500} \right) \times 30 \\
			   & = 32.94
		\end{align*}
		$$`
</script>
</section>
<section data-markdown><script type="text/template">
## Mean, Median, and Mode (3/4)
- Mode
	- The mode for a set of data is **the value that occurs most frequently** in the set. Therefore, it can be determined
	 for **qualitative and quantitative attributes**.
	- Data sets with one, two, or three modes are respectively called **unimodal**, **bimodal**, and **trimodal**.
- Example 2.8 Mode.  From example 2.6, the values for salary (in thousands of dollars), shown
	in increasing order: 30, 36, 47, 50, **52, 52**, 56, 60, 63, **70, 70**, 110, are **bimodal**. The two modes are 52,000 and 70,000.
</script></section>
<section data-markdown><script type="text/template">
## Mean, Median, and Mode (4/4)
- Midrange
	- The midrange can also be used to assess the central tendency of a numeric data set. It is the **average of the largest and smallest values** in the set.
- Example 2.9 Midrange. <br /> The midrange of the data of Example 2.6 is `$$\frac{30,000 + 110,000}{2} = 70,000$$`
</script></section>
<section data-markdown><script type="text/template">
## Symmetric vs. Skewed Data
![](/images/datamining/symmetric.png)
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Measuring the Dispersion of Data
</script></section>
<section data-markdown><script type="text/template">
<figure style="float: right">
<img src="/images/datamining/quartiles.png" alt="quartiles figure">
</figure>

## Range, Quartiles, and Interquartile Range
- Let `$x_1,x_2, \cdots ,x_N$` be a set of observations for some numeric attribute, `$X$`. **The range** of the set is the **difference** between the largest (**max()**) and smallest (**min()**) values.
- The **first quartile**, denoted by **`$Q_1$`**, is the **25th percentile**.
- The **third quartile**, denoted by **`$Q_3$`**, is the **75th percentile**—it cuts off the lowest 75% (or highest 25%) of the data.
- This distance is called the **interquartile range (IQR)** and is defined as: `$IQR = Q_3 - Q_1$`
</script></section>
<section data-markdown><script type="text/template">
## Example of Interquartile Range
- Example 2.10 Interquartile range. The quartiles are the three values that split the sorted data set into
four equal parts. The data of Example 2.6 contain **12 observations**(increasing order: 30, 36, **47**, 50, 52, **52**, 56, 60, **63**, 70, 70, 110,), already sorted in
increasing order. Thus, the quartiles for this data are the **third, sixth, and ninth** values,
respectively, in the sorted list. Therefore, **`$Q_1 = \$47,000$`** and **`$Q_3 = \$63,000$`**. Thus,
the interquartile range is `$IQR = 63 - 47 = \$16,000$`. (Note that the **sixth value is a
median, `$\$52,000$`**, although this data set has two medians since the number of data values
is even.)
</script></section>

<section data-markdown><script type="text/template">
## Quantiles
- Quantiles (分位數) are points taken at regular intervals of a data distribution, **dividing it into essentially equalsize** consecutive sets.
- The `$kth\; q-quantile$` for a given data distribution are less than `$x$` and at most `$(q - k)/ = q$` of the data values are more than `$x$`.
	- where `$k$` is an integer such that `$0 < k < q$`.(There are`$q-1\; q-quantile$`)
</script></section>

<section data-markdown><script type="text/template">
## Quantiles and Quartiles
- The 2-quantile (2 等分) is the data point dividing the lower and upper halves of the data distribution. It corresponds to the **median**.
<figure style="float: right">
<img src="/images/datamining/quartiles.png" alt="quartiles figure">
</figure>
- The 4-quantiles (4 等分) are the three data points that split the data distribution into four equal parts; 
	each part represents one-fourth of the data distribution. They are more commonly referred to as **quartiles**.
- The 100-quantiles are more commonly referred to as **percentiles**; they divide the data distribution into **100 equal-sized** consecutive sets.

</script></section>
<section data-markdown><script type="text/template">
## Five-Number Summary and Outliers
- The **five-number summary** of a distribution consists of the `$median (Q_2)$`, the `$quartiles \;Q_1$` and `$Q_3$`, and the **smallest** and **largest** individual observations,
written in the order of **`$Minimum$`, `$Q_1$`, `$Median$`, `$Q_3$`, `$Maximum$`**.
- A common rule of thumb for identifying suspected **outliers** is to single out values falling at least **`$1.5\times IQR$` above** the third quartile or **below the first
quartile**.
	- Below the `$ Outliers = Q_{1} - 1.5 \times IQR$` 
	- Above the `$ Outliers = Q_{3} + 1.5 \times IQR$`
</script></section>

<section data-markdown><script type="text/template">
## Boxplots (1/2)
- Boxplots are a popular way of visualizing a distribution. A boxplot incorporates the **five-number summary** as follows:
	- Typically, the ends of the box are at the quartiles so that **the box length is the interquartile range**.
	- The **median** is marked by **a line within the box**.
	- Two lines (**called whiskers**) outside the box extend to the smallest (Minimum) and largest (Maximum) observations.
</script></section>
<section data-markdown><script type="text/template">
## Boxplots (2/2)
![](/images/datamining/boxplot.png)
</script></section>

<section data-markdown><script type="text/template">
## Variance and Standard Deviation
- Variance
	- The variance of `$N$` observations, `$x_1 , x_2 , \cdots , x_N$`, for a numeric attribute `$X$` is
	`$$ \sigma^2 = \frac{1}{N} \sum^{N}_{i=1} (x_1 - \overline{x})^2 = \left(\frac{1}{N} \sum^{N}_{i=1} x_1^2 \right) - \overline{x}^2 $$`
	- where `$\overline{x}$` is the **mean** value of the observations, as defined in Eq. (2.1).
- The **standard deviation, `$\sigma$`**, of the observations is the **square root of the variance**, `$\sigma^2$` .
</script></section>

</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Measuring Data Similarity and Dissimilarity
</script></section>

<section data-markdown><script type="text/template">
## Similarity and Dissimilarity
- In data mining applications, such as **clustering, outlier analysis, and nearest-neighbor classification**, 
	we need ways to assess how **alike** or **unalike** objects are in comparison to one another.
- A cluster a collection of data objects such that **the objects within a cluster are similar to one another** 
	and **dissimilar** to the objects in other clusters.
- **Outlier analysis** also employs clustering-based techniques to identify potential outliers as objects that are **highly dissimilar** to others.
- Knowledge of object similarities can also be used in **nearest-neighbor classification** schemes.
</script></section>
<section data-markdown><script type="text/template">
## Alike and Unalike
- Similarity and dissimilarity measures, which are referred to as **measures of proximity**.
- A similarity measure for two objects, i and j, will typically return **the value 0 if the objects are unalike**.
- The **higher** the similarity **value**, the **greater the similarity** between objects.
- Typically, a **value of 1** indicates **complete similarity**, that is, the **objects are identical**.
</script></section>
<section data-markdown><script type="text/template">
## Data matrix (1/2)
- Suppose that we have **`$n$` objects** (e.g., persons, items, or courses) described by **`$m$` attributes** (also called **measurements** or **features**, such as age, height, weight, or gender).
- The objects are `$x_1 = (x_{11}, x_{12}, \cdots , x_{1m}), x_2 = (x_{21}, x_{22}, \cdots , x_{2m}),$` and so on, where `$x_{ij}$` is the value for object `$x_i$` of the `$jth$` attribute.
- The objects may be **tuples in a relational database**, and are also referred to as **data samples** or **feature vectors**.
</script></section>
<section data-markdown><script type="text/template">
## Data matrix (2/2)
- Data matrix is also called object-by-attribute structure. This structure stores the n data objects
in the form of a relational table, or n-by-m matrix (`$n$` objects `$\times$` `$m$` attributes)
`$$
\begin{bmatrix}
x_{11} &  \dots & x_{1f} & \dots  & x_{1m} \\
\dots  & \dots & \dots & \dots & \dots \\
x_{i1} &  \dots  & x_{if} & \dots  & x_{im} \\
\dots & \dots & \dots & \dots & \dots \\
x_{n1} &   \dots  & x_{nf} & \dots  & x_{nm}
\end{bmatrix}
$$`
</script></section>
<section data-markdown><script type="text/template">
## Dissimilarity matrix (1/2)
- Dissimilarity matrix is also called object-by-object (`$n\times n$` object) structure. This structure stores a collection of proximities 
that are available for all pairs of n objects.
`$$
\begin{bmatrix}
0 &  ~ & ~ & ~  & ~ \\
d(2,1)  & 0 & ~ & ~ & ~ \\
d(3,1) &  d(3,2)  & 0 & ~  & ~ \\
\vdots & \vdots & \vdots & ~ & ~ \\
d(n,1) &   d(n,2)  & \dots & \dots  & 0
\end{bmatrix}
$$`
where `$d(i, j)$` is the measured **dissimilarity** or “difference” between objects `$i$` and `$j$`.
</script></section>
<section data-markdown><script type="text/template">
## Dissimilarity matrix (2/2)
- In general, `$d(i, j)$` is a **non-negative number** that is **close to 0** when objects `$i$` and `$j$` are **highly similar** 
or “near” each other, and becomes larger the more they differ.
- Note that `$d(i, j)$`; that is, the **difference between an object and itself** is 0
- Furthermore, `$d(i, j) = d(j, i)$`; (For **readability**, we **do not show** the `$d(i, j)$` entries; the matrix is **symmetric**.)

</script></section>
<section data-markdown><script type="text/template">
## Data v.s Dissimilarity Matrix
- Measures of similarity can often **be expressed as a function** of measures of dissimilarity.
For example, for nominal data, **`$ sim(i, j) = 1 - d(i, j)$`**
- A **data matrix** is made up of two entities or “things,” namely rows (for objects) and columns (for attributes). 
Therefore, the data matrix is often called a **two-mode matrix**.
- The **dissimilarity matrix** contains one kind of entity (dissimilarities) and so is called a **one-mode matrix**. 
Many **clustering** and **nearest-neighbor** algorithms operate on a dissimilarity matrix.
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Proximity Measures for<br/> Nominal Attributes
</script></section>
<section data-markdown><script type="text/template">
## Dissimilarity between nominal attributes
- A nominal attribute can take on **two or more states**. For example, **map color** is a **nominal attribute** that may have,
 say, **five states: red, yellow, green, pink, and blue**.
 `$$
  d(i, j) = \frac{p - m}{p}
 $$`
	- where **m is the number of matches** (i.e., the number of attributes for which i and j are in the same state)
	- p is the **total number of attributes** describing the objects.
</script></section>
<section data-markdown><script type="text/template">
## Example of Dissimilarity between nominal attributes
- Suppose that we have the sample data of Table 2.2, except that only the object-identifier and the attribute test-1 are available,
where **test-1 is nominal**.

<figure style="float: right">
<img src="/images/datamining/table22.png" alt="example 2.17">
</figure>
</script></section>
<section data-markdown><script type="text/template">
## Continue of prior slides (1/2)
- Let\'s compute the dissimilarity matrix, that is,
`$
\begin{bmatrix}
0 &  ~ & ~ & ~ \\
d(2,1)  & 0 & ~ & ~  \\
d(3,1) &  d(3,2)  & 0 & ~  \\
d(4,1) &   d(4,2)  &  d(4,3) & 0 
\end{bmatrix}
$`
<figure style="float: right">
<img src="/images/datamining/table22small.png" alt="example 2.17 test-1">
</figure>
-  **`$d(i, j) = \frac{p - m}{p}$`**
	- we have one nominal attribute, test-1, we set **`$p = 1$`**
	- **`$m$` is the number of matches.** 
	- `$m_{(2,1)} = 0,\; d(2,1) = \frac{1-0}{1} = 1 $`
</script></section>
<section data-markdown><script type="text/template">
## Continue of prior slides (2/2)
-  `$d(i, j) = \frac{p - m}{p}$`
<figure style="float: right">
<img src="/images/datamining/table22small.png" alt="example 2.17 test-1">
</figure>
- `$m_{(3,1)} = 0,\; d(3,1) = \frac{1-0}{1} = 1 $`
- `$m_{(3,2)} = 0,\; d(3,2) = \frac{1-0}{1} = 1 $`
- `$m_{(4,1)} = 1,\; d(4,1) = \frac{1-1}{1} = 0 $`
- `$m_{(4,2)} = 0,\; d(4,2) = \frac{1-0}{1} = 1 $`
- `$m_{(4,3)} = 0,\; d(4,3) = \frac{1-0}{1} = 1 $`
- Let\'s compute the dissimilarity matrix, that is,
`$
\begin{bmatrix}
0 &  ~ & ~ & ~ \\
1  & 0 & ~ & ~  \\
1 &  1  & 0 & ~  \\
0 &   1  &  1 & 0 
\end{bmatrix}
$`
</script></section>
<section data-markdown><script type="text/template">
## Two nominal attributes (1/2)
| Object | test-1 | test-2 |
|:---:		| :---: | :---: |
| 1		| code A | 20 |
| 2		| code B | 30 |
| 3		| code C | 40 |
| 4 	| code A | 20 |
- we have two nominal attributes, test-1 and test-2, we set **`$p = 2$`**
- `$m_{(2,1)} = 0,\; d(2,1) = \frac{2-0}{2} = 1 $`
- `$m_{(4,1)} = 2,\; d(4,1) = \frac{2-0}{2} = 0 $`
</script></section>
<section data-markdown><script type="text/template">
## Two nominal attributes (2/2)
| Object | test-1 | test-2 |
|:---:		| :---: | :---: |
| 1		| code A | 20 |
| 2		| code B | 30 |
| 3		| code C | 40 |
| 4 	| code A | 20 |
- Let\'s compute the dissimilarity matrix, that is,
`$$
\begin{bmatrix}
0 &  ~ & ~ & ~ \\
1  & 0 & ~ & ~  \\
1 &  1  & 0 & ~  \\
0 &   1  &  1 & 0 
\end{bmatrix}
$$`
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Proximity Measures for<br/> Binary Attributes
</script></section>
<section data-markdown><script type="text/template">
## Dissimilarity between two binary attributes
- A **binary attribute** has only one of **two states: 0 and 1**, where **0 means** that the attribute is **absent**, and **1 means** that it is **present**.
- **How can we compute** the dissimilarity between two binary attributes as below table?
| name		| gender	| fever 	| cough	| att1 | att2 	| att3| att4 | 
|:---:		| :---: 	| :---: 	|:---: 	| :---:  |:---: 	| :---: | :---:  |
| Jack		| M 		| Y 		| N		| P		 | N		| N		| N		 |
| Jim		| M 		| Y 		| Y 	| N		 | N		| N		| N		 |
| Mary		| F 		| Y 		| N		| P		 | N		| P		| N		 |
| `$\vdots$`| `$\vdots$`| `$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|
</script></section>
<section data-markdown><script type="text/template">
## Contingency table (1/2)
- We have the `$2\times 2$` contingency table, where **`$q$` is the number of attributes** that **equal 1 for both objects**
`$i$` and `$j$` 
![](/images/datamining/contingency.png)
</script></section>
<section data-markdown><script type="text/template">
## Contingency table (2/2)
- `$r$` is the number of attributes that **equal 1 for object `$i$` but equal 0 for object `$j$`.** 
`$
\begin{array}{c|cc}
   ~ & 1 & 0 \\ \hline
   1 & q & r \\
   0 & s & t
\end{array}
$`
`$ =
\begin{array}{c|cc}
   ~ & 1 & 0 \\ \hline
   1 & m_{11} & m_{10} \\
   0 & m_{01} & m_{00}
\end{array}
$`
- `$s$` is the number of attributes that equal 0 for object `$i$` but equal 1 for object `$j$`
- `$t$` is the number of attributes that equal 0 for both objects `$i$` and `$j$`.
- The total number of attributes is `$p$`, where **`$p = q + r + s + t$`**.
</script></section>
<section data-markdown><script type="text/template">
## Symmetric binary dissimilarity
- Recall that for **symmetric binary attributes, each state is equally valuable**.
- Dissimilarity that is based on **symmetric binary attributes** is called **symmetric binary dissimilarity**.
- If objects i and j are described by symmetric binary attributes, then the dissimilarity between i and j is
`$$
d(i, j) = \frac{r + s}{q + r + s + t}
$$`
</script></section>
<section data-markdown><script type="text/template">
## Asymmetric binary dissimilarity
- For **asymmetric binary attributes**, the two states are **not equally important**, such as the **positive (1) and negative (0)** 
  outcomes of a **disease test**.
- If objects `$i$` and `$j$` are described by symmetric binary attributes, then the dissimilarity between `$i$` and `$j$` is
`$$
d(i, j) = \frac{r + s}{q + r + s}
$$`
- The asymmetric binary similarity is
`$$
sim(i, j) = 1 - \frac{r + s}{q + r + s} = \frac{q}{q + r + s}
$$`
</script></section>
<section data-markdown><script type="text/template">
## Example of dissimilarity between binary attributes (1/5)
- Suppose that a patient record table as following
| name		| gender	| fever 	| cough	| att1 | att2 	| att3| att4 | 
|:---:		| :---: 	| :---: 	|:---: 	| :---:  |:---: 	| :---: | :---:  |
| Jack		| M 		| Y 		| N		| P		 | N		| N		| N		 |
| Jim		| M 		| Y 		| Y 	| N		 | N		| N		| N		 |
| Mary		| F 		| Y 		| N		| P		 | N		| P		| N		 |
| `$\vdots$`| `$\vdots$`| `$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|`$\vdots$`|
- For asymmetric attribute values, let the values Y (yes) and P (positive) be set to 1, and the value N (no or negative) be set to 0.
</script></section>
<section data-markdown><script type="text/template">
## Example of dissimilarity between binary attributes (2/5)
| name		| gender	| fever 	| cough	| att1 | att2 	| att3| att4 | 
|:---:		| :---: 	| :---: 	|:---: 	| :---:  |:---: 	| :---: | :---:  |
| Jack		| M 		| Y 		| N		| P		 | N		| N		| N		 |
| Jim		| M 		| Y 		| Y 	| N		 | N		| N		| N		 |
| Mary		| F 		| Y 		| N		| P		 | N		| P		| N		 |

| name		|  fever 	| cough	| att1 | att2 	| att3| att4 | 
|:---:		| :---: 	|:---: 	| :---:  |:---: 	| :---: | :---:  |
| Jack		| 1 		| 0		| 1		 | 0		| 0		| 0		 |
| Jim		| 1 		| 1 	| 0		 | 0		| 0		| 0		 |
| Mary		| 1 		| 0		| 1		 | 0		| 1		| 0		 |
</script></section>
<section data-markdown><script type="text/template">
## Example of dissimilarity between binary attributes (3/5)
- Suppose that the distance between objects (patients) is computed based only on the asymmetric attributes.
- the distance between each pair of the three patients—Jack, Mary, and Jim—is
`$$
\begin{array}{c|c:c:c}
   ~ & Jack & Jim & Mary\\ \hline
   Jack & 0 & ~ & ~ \\
   Jim & d_{21} & 0 & ~\\
   Mary & d_{31} & d_{32} & 0
\end{array}
$$`
</script></section>
<section data-markdown><script type="text/template">
## Example of dissimilarity between binary attributes (4/5)
- `$d(Jack, Jim) = \frac{r + s}{q + r + s}  = \frac{m_{10} + m_{01}}{m_{11} + m_{10} + m_{01}}$`
- **`$m_{10} \mapsto$` Jack = 1, Jim = 0;** `$m_{01} \mapsto$` Jack = 0, Jim = 1; `$m_{11} \mapsto$` Jack = 1, Jim = 1;
| name		|  fever 	| cough	| att1 | att2 	| att3| att4 | 
|:---:		| :---: 	|:---: 	| :---:  |:---: 	| :---: | :---:  |
| Jack		| 1 		| 0		| 1		 | 0		| 0		| 0		 |
| Jim		| 1 		| 1 	| 0		 | 0		| 0		| 0		 |
| Mary		| 1 		| 0		| 1		 | 0		| 1		| 0		 |
</script></section>
<section data-markdown><script type="text/template">
## Example of dissimilarity between binary attributes (5/5)
`$$
\begin{align*}
d(Jack, Jim) & = \frac{r + s}{q + r + s}  = \frac{m_{10} + m_{01}}{m_{11} + m_{10} + m_{01}}\\
	& = \frac{1 + 1}{ 1 + 1 + 1}  = 0.67\\
\end{align*}
$$`
`$$
\begin{align*}
d(Jack, Mary)  & = \frac{m_{10} + m_{01}}{m_{11} + m_{10} + m_{01}} = \frac{0 + 1}{ 2 + 0 + 1}  = 0.33\\
\end{align*}
$$`
`$$
\begin{align*}
d(Jim, Mary)  & = \frac{m_{10} + m_{01}}{m_{11} + m_{10} + m_{01}} = \frac{1 + 2}{ 1 + 1 + 2}  = 0.75\\
\end{align*}
$$`
</script></section>
</section><!--end block-->


<section><!--start block-->
<section data-markdown><script type="text/template">
# Proximity Measures for<br/> Numeric Attributes
</script></section>
<section data-markdown><script type="text/template">
## Distance measures
- we describe **distance measures** that are commonly used for computing the **dissimilarity** of objects described by **numeric attributes**.
- These measures include the **Euclidean**, **Manhattan**, and **Minkowski** distances.
- In some cases, the **data are normalized before applying** distance calculations.
	- This involves **transforming the data to fall within a smaller** or **common range**, such as [-1, 1]	or [0.0, 1.0].
</script></section>
<section data-markdown><script type="text/template">
## The Euclidean distance
- Let `$i = x_{i1}, x_{i2}, \cdots , x_{ip}$` and `$j = x_{j1}, x_{j2}, \cdots , x_{jp}$` be two objects described by `$p$` numeric attributes.
- Euclidean distance between objects `$i$` and `$j$` is defined as
`$$
 d(i, j) = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \cdots + (x_{ip}-x_{jp})^2}
$$`
1. **Non-negativity**: `$d(i, j) \geq 0 \blacktriangleright$` Distance is a non-negative number.
2. **Identity of indiscernibles**: `$d(i, j) = 0 \blacktriangleright$` The distance of an object to itself is 0.
</script></section>
<section data-markdown><script type="text/template">
## The Manhattan distance
- The Manhattan distance is also called **city block distance**.
- Let `$i = x_{i1}, x_{i2}, \cdots , x_{ip}$` and `$j = x_{j1}, x_{j2}, \cdots , x_{jp}$` be two objects described by `$p$` numeric attributes.
- The Manhattan distance is defined as
`$$
 d(i, j) = |x_{i1}-x_{j1}| + |x_{i2}-x_{j2}| + \cdots + |x_{ip}-x_{jp}|
$$`
3. **Symmetry**: `$d(i, j) = d(j, i)\blacktriangleright$` Distance is a symmetric function.
4. **Triangle inequality**: `$d(i, j) \leq d(i, k) + d(k, j) \blacktriangleright$` Going directly from object `$i$` to object `$j$`
in space is no more than making a detour over any other object `$k$`.
</script></section>
<section data-markdown><script type="text/template">
## Mathematical properties of Euclidean and Manhattan
1. **Non-negativity**: `$d(i, j) \geq 0 \blacktriangleright$` Distance is a non-negative number.
2. **Identity of indiscernibles**: `$d(i, j) = 0 \blacktriangleright$` The distance of an object to itself is 0.
3. **Symmetry**: `$d(i, j) = d(j, i)\blacktriangleright$` Distance is a symmetric function.
4. **Triangle inequality**: `$d(i, j) \leq d(i, k) + d(k, j) \blacktriangleright$` Going directly from object `$i$` to object `$j$`
in space is no more than making a detour over any other object `$k$`.
</script></section>
<section data-markdown><script type="text/template">
## The Minkowski distance
- Minkowski distance is a generalization of the Euclidean and Manhattan distances.
- The Minkowski distance is defined as
`$$
 d(i, j) = \sqrt[h]{|x_{i1}-x_{j1}|^h + |x_{i2}-x_{j2}|^h + \cdots + |x_{ip}-x_{jp}|^h}
$$`
 - where **`$h$` is a real number** such that **`$h \geq 1$`**.(Such a distance is also called `$L_p$` **norm** in some literature)
 - the Manhattan distance when **`$h = 1$`** (i.e., `$L_1$` **norm**) and Euclidean distance when **`$h = 2$`**  (i.e., `$L_2$` **norm**).
</script></section>
<section data-markdown><script type="text/template">
## The supremum distance
- The supremum distance is also called Chebyshev distance.
- It is a generalization of the Minkowski distance for `$h \longrightarrow\infty$`, which also referred to as `$L_{max}, L_{\infty}$` **norm**.
- To compute it, we find the attribute `$f$` that gives the **maximum difference in values between the two objects**.
- This difference is the supremum distance, defined more formally as:
`$$
d(i, j) = \lim_{h \to \infty} \left( \sum_{f = 1}^{p} |x_{if} - x_{jf}|^h \right) 
$$`
</script></section>
<section data-markdown><script type="text/template">
## An example for distance measure
- Let `$x_1 = (1, 2)$` and `$x_2 = (3, 5)$` represent two objects as shown in Figure.
- The Euclidean distance between the two is `$ \sqrt {(3-1)^2 + (5 - 2)^2} = \sqrt {2^2 + 3^2} = 3.61$`
<figure style="float: right">
<img src="/images/datamining/distance.png" alt="distance example">
</figure>
- The Manhattan distance between the two is `$ |3-1| + |5-2| = 2 + 3 = 5$`
- The second attribute gives the greatest difference between values for the objects, which is `$5 - 2 = 3$`. 
This is the supremum distance between both objects.
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Proximity Measures for<br/> Ordinal Attributes
</script></section>
<section data-markdown><script type="text/template">
## Ordinal Attribute
- **The values of an ordinal attribute** have a **meaningful order** or **ranking** about them, yet the magnitude between successive **values is unknown**.
- For example, the range of the **interval-scaled** attribute temperature (in _Celsius_)can be organized into the following 
	states: `$-30 \;to\; -10, -10 \;to\; 10, 10 \;to\; 30$`, representing the categories cold _temperature, moderate temperature_, and _warm_ temperature,
respectively.
</script></section>
<section data-markdown><script type="text/template">
## 
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Proximity Measures for<br/> Mixed Types Attribute
</script></section>
<section data-markdown><script type="text/template">
## 
</script></section>
</section><!--end block-->

</div>
</div>

<script src="/revealjs/lib/js/head.min.js"></script>
<script src="/revealjs/dist/reveal.js"></script>
<script src="/revealjs/plugin/zoom/zoom.js"></script>
<script src="/revealjs/plugin/notes/notes.js"></script>
<script src="/revealjs/plugin/search/search.js"></script>
<script src="/revealjs/plugin/markdown/markdown.js"></script>
<script src="/revealjs/plugin/highlight/highlight.js"></script>
<script src="/revealjs/plugin/menu/menu.js"></script>
<script src="/revealjs/plugin/math/math.js"></script>
<script src="/revealjs/plugin/chalkboard/plugin.js"></script>
<script src="/revealjs/plugin/customcontrols/plugin.js"></script>
<script src="/revealjs/plugin/animate/svg.min.js"></script>
<script src="/revealjs/plugin/animate/plugin.js"></script>

<script>
Reveal.initialize({
controls: true,
progress: true,
history: true,
center: true,
slideNumber: true,
mouseWheel: true,
transition: 'slide', // none/fade/slide/convex/concave/zoom

menu: {
	side: 'left',
	width: 'normal',
	numbers: false,
	titleSelector: 'h1, h2, h3, h4, h5, h6',
	useTextContentForMissingTitles: false,
	hideMissingTitles: false,
	markers: true,
	custom: true,
	themes: true,
	themesPath: '/revealjs/dist/theme/',
	transitions: true,
	openButton: true,
	openSlideNumber: false,
	keyboard: true,
	sticky: false,
	autoOpen: true,
	delayInit: false,
	openOnInit: false,
	loadIcons: true,
	
	custom: [
			{ title: 'TOC', icon: '<i class="fa fa-external-link-alt">', src: 'links.html' },
			{ title: 'About', icon: '<i class="fa fa-info">', content: '<p>Slides for teaching Office Suite Softwar</p>' }
	]
},

customcontrols: {
	controls: [
	{
		id: 'toggle-overview',
		title: 'Toggle overview (O)',
		icon: '<i class="fa fa-th"></i>',
		action: 'Reveal.toggleOverview();'
	},
	{ icon: '<i class="fa fa-pen-square"></i>',
		title: 'Toggle chalkboard (B)',
		action: 'RevealChalkboard.toggleChalkboard();'
	},
	{ icon: '<i class="fa fa-pen"></i>',
		title: 'Toggle notes canvas (C)',
		action: 'RevealChalkboard.toggleNotesCanvas();'
	}
	]
},

toolbar: {
	// Specifies where the toolbar will be shown: 'top' or 'bottom'
	position: 'bottom',

	// Add button to toggle fullscreen mode for the presentation
	fullscreen: true,

	// Add button to toggle the overview mode on and off
	overview: true,

	// Add button to pause (hide) the presentation display
	pause: true,

	// Add button to show the speaker notes
	notes: false,

	// Add button to show the help overlay
	help: false,

	// If true, the reveal.js-menu will be moved into the toolbar.
	// Set to false to leave the menu on its own.
	captureMenu: true,

	// If true, the playback control will be moved into the toolbar.
	// This is only relevant if the presentation is configured to autoSlide.
	// Set to false to leave the menu on its own.
	capturePlaybackControl: true,

	// By default the menu will load it's own font-awesome library
	// icons. If your presentation needs to load a different
	// font-awesome library the 'loadIcons' option can be set to false
	// and the menu will not attempt to load the font-awesome library.
	loadIcons: true
},

// Optional reveal.js plugins
plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealAnimate, RevealMenu, RevealCustomControls, RevealChalkboard, RevealMath.KaTeX ],

dependencies: [
//{ src: '/revealjs/plugin/toolbar/toolbar.js' }
]
});

</script>

</body>
</html>
