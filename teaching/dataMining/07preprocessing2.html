<!doctype html>
<html lang="zh-Hant-TW">

<head>
<meta charset="utf-8">

<title>資料預處理2 (Data Preprocessing 2)</title>

<meta name="description" content="Tutorial of web design for primer">
<meta name="author" content="Elvis Hsieh">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

<link rel="stylesheet" href="/revealjs/dist/reset.css">
<link rel="stylesheet" href="/revealjs/dist/reveal.css">
<link rel="stylesheet" href="/revealjs/dist/theme/black.css" id="theme">
<link rel="stylesheet" href="/revealjs/plugin/chalkboard/style.css">
<link rel="stylesheet" href="/revealjs/plugin/customcontrols/style.css">
<link rel="stylesheet" href="/css/custom4revealjs.css">
<!-- Code syntax highlighting -->
<link rel="stylesheet" href="/highlightjs/styles/monokai.min.css">   
<link rel="stylesheet" href="/css/style4font.css">
<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /revealjs/print-pdf/gi ) ? '/revealjs/dist/print/pdf.css' : '/revealjs/dist/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<!--[if lt IE 9]>
<script src="reveal-js/lib/js/html5shiv.js"></script>
<![endif]-->
</head>
<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">

<section><!--start block-->
<section data-markdown><script type="text/template">
# Data Reduction
</script></section>
<section data-markdown><script type="text/template">
## Data Reduction Strategies (1/2)
- Data reduction: Obtain a reduced representation of the data set that is much smaller in volume but yet produces the same (or almost the same) analytical results
- Why data reduction? — A database/data warehouse **may store terabytes of data**.  Complex data analysis may take a **very long time** to run on the complete data set.
- Data reduction strategies
	- **Dimensionality reduction**, e.g., remove unimportant attributes
		- Wavelet transforms
		- Principal Components Analysis (PCA)
		- Feature subset selection, feature creation
</script></section>
<section data-markdown><script type="text/template">
## Data Reduction Strategies (2/2)
- Data reduction strategies
	- **Numerosity reduction** (some simply call it: Data Reduction)
		- Regression and Log-Linear Models
		- Histograms, clustering, sampling
		- Data cube aggregation
	- **Data compression**
		- Lossless compression
		- Lossy compression
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Dimensionality reduction(減少維度)
</script></section>
<section data-markdown><script type="text/template">
## Dimensionality  (1/2)
- Cause of dimensionality
	- When **dimensionality increases**, data becomes increasingly sparse
	- **Density and distance between points**, which is **critical** to clustering, outlier analysis, becomes less meaningful
	- The possible combinations of subspaces will grow exponentially
- Dimensionality reduction
	- **Avoid** the curse of dimensionality
	- Help **eliminate irrelevant features** and **reduce noise**
</script></section>
<section data-markdown><script type="text/template">
## Dimensionality  (2/2)
- Dimensionality reduction
	- **Reduce time and space** required in data mining
	- Allow **easier visualization**
- Dimensionality **reduction techniques**
	- Wavelet transforms
	- Principal Component Analysis
	- Supervised and nonlinear techniques (e.g., feature selection)
</script></section>
<section data-markdown><script type="text/template">
## Mapping Data to a New Space
- Fourier transform
- Wavelet transform 
![](/images/datamining/waves.png)
</script></section>

<section data-markdown><script type="text/template">
# Fourier Transform
</script></section>
<section data-markdown><script type="text/template">
## Fourier Series
- A Fourier series is an expansion of a periodic function
<figure style="float: right; width: 155pt">
<img src="/images/datamining/fourierSeries.png" alt="Fourier Series">
</figure>
into a sum of trigonometric functions.

	- A periodic function is a function that **repeats** its values **at regular intervals**.
	- A Fourier series is **an infinite series form**.
	`$$a_0 + \sum^{\infty}_{n = 1}  a_n sin(n\omega x) + b_n cos(n\omega x)$$` 
![](/images/datamining/periodic.gif)
</script></section>
<section data-markdown><script type="text/template">
## Fourier transform
- The Fourier transform (FT) is a transform that converts a function into a form that describes the **frequencies present** in the original function.
	- The Fourier transform of a function of `$t$` gives a function of `$\omega$` where `$\omega$` is the angular frequency.
	`$$\tilde{f}(\omega) = \mathscr{F}(\omega) = \int^{\infty}_{-\infty} f(t)e^{-i2\pi\omega t}dt$$`
	- The Fourier inversion transform
	`$$ f(t) = \int^{\infty}_{-\infty} \tilde{f}(\omega)e^{i2\pi\omega t}d\omega$$`
</script></section>
<section data-markdown><script type="text/template">
## Fourier Transform with Python
<pre><code data-trim data-line-numbers>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import EngFormatter
x = np.linspace(0,5,100)
y = np.sin(2*np.pi*x)
## fourier transform
f = np.fft.fft(y)
## sample frequencies
freq = np.fft.fftfreq(len(y), d=x[1]-x[0])
# plot figure
plt.figure(figsize=(15,5))
ax1 = plt.subplot(1,2,1)
ax1.plot(y, label="y = sin(2*pi*x)")
plt.xlabel('Number of Sample')
plt.ylabel('Magnitude Value')
ax1.legend(loc="upper right")
ax2 = plt.subplot(1,2,2)
ax2.plot(freq, abs(f)**2, label="Fourier Transform", color='r')
formatter0 = EngFormatter(unit='Hz')
ax2.xaxis.set_major_formatter(formatter0)
plt.xlabel('Frequencies [Hertz]')
plt.ylabel('Amplitude')
ax2.legend(loc="upper right")
</code></pre>
</script></section>
<section data-markdown><script type="text/template">
## The result of above codes
![](/images/datamining/ft.png)
</script></section>

<section data-markdown><script type="text/template">
# Haar Wavelet
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Transform
- The simplest and earliest example of a wavelet is the **Haar wavelet, developed in 1910**.
`$$\psi (t) = \begin{cases}
	1  & 0 \leq t < \frac{1}{2} \\
	-1 & \frac{1}{2} \leq t < 1 \\
	0  & otherwise.
	\end{cases}
$$`
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Diagram(1/2)
![](/images/datamining/haarD.png)
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Diagram(2/2)
<pre><code data-trim data-line-numbers>import numpy as np
import matplotlib.pyplot as plt
def haar(t):
	if 0 < t < 0.5:
		return 1
	elif 0.5 < t < 1:
		return -1
	else:
		return 0
t = np.linspace(0, 1, 1000) # Create a time vector from 0 to 1 with 1000 points
x_values = np.array([haar(ti) for ti in t]) # Evaluate haar(t) each time point
# Plot the haar(t) function
plt.plot(t, x_values, linestyle='dotted', color='blue')
plt.xlabel('X axis: Time Domain')
plt.ylabel('Y axis: Amplitude')
plt.title('Haar Wavelet Function')
# Set the y-axis limits
plt.ylim(-1.2, 1.2)
plt.axhline(y=0, color='k', linewidth=0.5) # Draw the x-axis baseline on 0
plt.plot([0, 0.5], [1, 1], linewidth=2, label='t = Positivie', color='blue',linestyle='solid')
plt.plot([0.5, 1], [-1, -1], linewidth=2, label='t = Negativie', color='darkblue',linestyle='solid')
plt.legend()
plt.show()</code></pre>
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Transform(1/4)
![](/images/dataMining/haar.png)
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Transform(2/4)
- X = [**2, 2, 0, 2, 3, 5, 4, 4**] can be transformed to `$\overline{X}$` = [2.75, -1.25, 0.5, 0, 0, -1, -1, 0] (兩兩相加除以2，兩兩相減除以2)
1. `$\overline{X_1}$` = (X[0] **+** X[1])/2 = 2, `$\overline{X_2}$` = ([2] **+** X[3])/2 = 1, 以此類推<br /> 
`$\overline{X_5}$` = (X[0] **-** X[1])/2 = 0, `$\overline{X_6}$` = ([2] **-** X[3])/2 = -1, 以此類推
2. 對 [**2, 1, 4, 4**, 0, -1, -1, 0] 紅色數值再兩兩進行 +/2 以及 -/2 的動作，得到 [**`$\frac{3}{2}$`, 4**, `$\frac{1}{2}$`, 0, 0, -1, -1, 0]
3. 相同上述步驟，最後得 [**`$\frac{11}{4}$`,`$-\frac{5}{4}$`, `$\frac{1}{2}$`, 0, 0, -1, -1, 0**]
![](/images/dataMining/haarTable.png)
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Transform(3/4)
![](/images/dataMining/haarInverse.png)
</script></section>
<section data-markdown><script type="text/template">
## Haar Wavelet Transform(4/4)
- PyWavelets: a Python package for wavelet transform
	- **pip install PyWavelets**
- **Haar** Wavelet Transform and **inverse** transform with **PyWavelets** package.
<pre><code data-trim data-line-numbers>import pywt
x = [3, 7, 1, 1, -2, 5, 4, 6]
cA, cD = pywt.dwt(x, 'haar') # Haar wavelet transform
y = pywt.idwt(cA, cD, 'haar') # inverse Haar wavelet transform
print(cA) # Approxmation coefficients
print(cD) # Detailed coefficients
print(y) # Reconstructed Signal
</code></pre>
</script></section>

<section data-markdown><script type="text/template">
# Wavelet Transforms
</script></section>
<section data-markdown><script type="text/template">
## What Is Wavelet Transform?
- Decomposes a signal into different frequency subbands
	- Applicable to n-dimensional signals
- Data are transformed to **preserve relative distance** between objects at **different levels of resolution**
- Allow natural clusters to **become more distinguishable(可區別)**
- Used for **image compression**
![](/images/datamining/avatar.png)
</script></section>
<section data-markdown><script type="text/template">
## Wavelet Transform(1/2)
- There are two types of Wavelet Transforms: Continuous Wavelet Transform and Discrete Wavelet Transform. 
	- Continuous Wavelet Transform(CWT) 
	`$${\mathscr{F}}(a, b) = \frac{1}{\sqrt{a}} \int^{\infty}_{-\infty} \Psi(\frac{t-b}{a})x(t)dt$$`
	 where `$a$` is scaling; `$b$` is time shift factor.
	- Discrete Wavelet Transform (DWT)
	`$${\mathscr{F}}_{j, k} = \frac{1}{\sqrt{a}} \int^{\infty}_{-\infty} \psi_{j,k}(t)x(t)dt, \; \psi_{j,k}(t) = \frac{1}{\sqrt{2^j}} \psi(\frac{t-k2^j}{2^j})$$`
</script></section>
<section data-markdown><script type="text/template">
## Wavelet Transform(2/2)
- Why wavelets?
	- Wavelets have some slight benefits over Fourier transforms in **reducing computations** 
	when examining specific frequencies. 
	- The common **Morlet wavelet** is mathematically **identical to a short-time Fourier 
	transform** using a Gaussian window function
![](/images/datamining/waveFig.svg)
</script></section>
<section data-markdown><script type="text/template">
## Wavelet Transforms family
- Wavelet transform has become a powerful tool for signal processing. It offers time-frequency analysis to decompose 
the signal in terms of a family of wavelets.
- The DWT is applied to a data vector `$X$`, transforms it to a numerically different vector, `$X'$`, of
wavelet coefficients. It are of the same length.
- “**How can this technique be useful for data reduction** if the wavelet transformed data are
of the same length as the original data?” The usefulness lies in the fact that **the wavelet transformed data can be truncated**.
- Reference: [Data Driven Science & Engineering Machine Learning, Dynamical Systems, and Contro](http://databookuw.com/databook.pdf)
</script></section>
<section data-markdown><script type="text/template">
## An example of DWT (1/2)
<pre><code data-trim data-line-numbers>from matplotlib.image import imread
import matplotlib.pyplot as plt
import numpy as np
import pywt
# Load image
original = pywt.data.camera() # image from PyWavelets
# A = imread("D:/Teaching/dataMining/dataset/images/avatar.png")
# original = np.mean(A, -1)
# print(original) # check point
titles = ['Approximation', ' Horizontal detail', 'Vertical detail', 'Diagonal detail']
coeffs2 = pywt.dwt2(original, 'bior1.3')
LL, (LH, HL, HH) = coeffs2
fig = plt.figure(figsize=(10, 10))
for i, a in enumerate([LL, LH, HL, HH]):
	ax = fig.add_subplot(1, 4, i + 1) 
	ax.imshow(a, interpolation="nearest", cmap=plt.cm.gray)
	ax.set_title(titles[i], fontsize=12)
	ax.set_xticks([])
	ax.set_yticks([])
fig.tight_layout()
# plt.savefig('D:/Teaching/dataMining/dataset/images/avatar{0}.png'.format(i), format='png')
plt.show()</code></pre>
</script></section>
<section data-markdown><script type="text/template">
## An example of DWT (2/2)
- pip install PyWavelets
![](/images/datamining/dwt.png)
</script></section>
<section data-markdown><script type="text/template">
## Discrete Wavelet Transform(1/4)
<pre><code data-trim data-line-numbers># 07_03wavelet.py part1
import numpy as np
import pywt
import matplotlib.image as img
import matplotlib.pyplot as plt
original = img.imread("D:/Teaching/dataMining/dataset/images/avatar.png") # should be a grayscale image
img = np.mean(original, -1) # color image(R,G,B) scale into grayscale image

coeffs2 = pywt.dwt2(img, 'db3', mode='periodization') # 1 level DWT
cA, (cH, cV, cD) = coeffs2 # UL, UR, LL, LR
imgR = pywt.idwt2(coeffs2, 'db3', mode='periodization') # 1 level iDWT
imgR = np.uint8(imgR)

plt.figure(figsize=(10, 10))
plt.rc('xtick', labelsize=10) 
plt.rc('ytick', labelsize=10)

plt.subplot(2,2,1)
plt.imshow(cA, cmap=plt.cm.gray)
plt.title("CA: Approximation Coeff.", fontsize=10)
</code></pre>
</script></section>
<section data-markdown><script type="text/template">
## Discrete Wavelet Transform(2/4)
<pre><code data-trim data-line-numbers># 07_03wavelet.py part2
plt.subplot(2,2,2)
plt.imshow(cH, cmap=plt.cm.gray)
plt.title("CH: Horizontal Detail Coeff.", fontsize=10)

plt.subplot(2,2,3)
plt.imshow(cV, cmap=plt.cm.gray)
plt.title("CV: 'Vertical Detail Coeff.", fontsize=10)

plt.subplot(2,2,4)
plt.imshow(cD, cmap=plt.cm.gray)
plt.title("CD: Diagonal Detail Coeff.", fontsize=10)

plt.show()

# plotting Reconstructed image
plt.imshow(imgR, cmap=plt.cm.gray)
plt.title("Reconstructed Image.", fontsize=10)</code></pre>
</script></section>
<section data-markdown><script type="text/template">
## Discrete Wavelet Transform(3/4)
![](/images/datamining/dwt2.png)
</script></section>
<section data-markdown><script type="text/template">
## Discrete Wavelet Transform(4/4)
![](/images/datamining/dwt3.png)
</script></section>

<section data-markdown><script type="text/template">
# Principal <br /> Component Analysis,PCA
</script></section>
<section data-markdown><script type="text/template">
## Principal Component Analysis
- 課本對於 Principal Component Analysis(PCA) 原理的描述篇幅不多，僅有幾個 PCA 的步驟，想要了解 PCA 原理的同學可以連結到下面的網址。
- [用最直观的方式告诉你：什么是主成分分析PCA](https://www.youtube.com/watch?v=DSS7MmhXjr8&t=402s)，
以中文、動畫的方式解說 PCA，非常適合給同學參考。
- PCA 使用到線性代數的觀念，如果對於線性代數的線性轉換(座標轉換)，可以參考中華大學李柏堅老師的[特徵向量與特徵值](https://www.youtube.com/watch?v=OGZX9CYYEic&t=357s)教學影片，瞭解特徵值(eigenvalue)、特徵向量(eigenvector)的觀念。
-  Datasets 經過 PCA 的轉換，可以投影到新的座標軸，如此就可以把 Datasets 的維度降階，達到 data reduction 的效果。
</script></section>
<section data-markdown><script type="text/template">
## How to compute PCA?
To compute a PCA, we can perform the following steps
1. **Center** or standardize of data which compute the **mean and dividing by the standard deviation** for each value of each data.
2. Compute the **covariance matrix** to identify correlations.
3. Compute the **eigenvectors and eigenvalues of the covariance matrix** to identify the principal components.
4. Create a **feature vector** to decide which **principal components to keep**.
5. Recast(重鑄) the data along the principal components axes.
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(1/5)
- Suppose we have two attributes `$X, Y$` which have been recorded `$[126, 128, 128, 130, 130, 132]$`  and  `$ [78, 80, 82, 82, 84, 86]$` observed values respectively. How to compute the PCA?
- **Step 1.** Centering data
	- `$\overline{x}_{(mean)} = \frac{126 + 128 + 128 + 130 + 130 + 132}{6} = 129$` 
	- `$\overline{y}_{(mean)} = \frac{78 + 80 + 82 + 82 + 84 + 86}{6} = 82$`
	- Center : `$x_i - \overline{x} = [-3_{(126-129)}, -1, -1, 1, 1, 3]$`, and `$y_i - \overline{y} = [-4_{(78-82)}, -2, 0, 0, 2, 4]$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(2/5)
- Center: `$[-3_{(x_1 - \overline{x})}, -1, -1, 1, 1, 3]$`, `$[-4, -2, 0, 0, 2, 4]$`
- **Step 2.** Compute the covariance matrix
	- Variance `$\sigma^2 = \frac{1}{(n-1)} \sum^{n}_{i=1} (x_1 - \overline{x})^2$`(樣本變異數), <br /> **$\sigma^2_{x} = 4.4$`, $\sigma^2_{y} = 8.0$`**
	- `$Cov(X,Y) = \frac{1}{(n-1)} \sum^n_{i=1} (x_i - \overline{x})(y_i - \overline{y}) $`(共變異數), 

	`$$
	\begin{align*}
	Cov(X,Y) &= \frac{1}{(6-1)} [(-3\times -4)+ 2 + 0 + 0 + 2 + 12] \\
	Cov(Y,X) &= \frac{28}{5} = \underline{5.6} 
	\end{align*}
	$$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(3/5)
- The covariance matrix is
`$
\begin{array}{c|cc}
   ~ & X & Y \\ \hline
   X & 4.4 & 5.6 \\
   Y & 5.6 & 8.0
\end{array}
$`
- **Step 3.** Compute the eigenvectors and eigenvalues of covariance matrix, i.e. `$det|A - \lambda| I = 0$`. 

`$ det \left |
\begin{bmatrix}
4.4 &  5.6 \\
5.6  & 8.0 
\end{bmatrix} -
\begin{bmatrix}
\lambda &  0 \\
0  & \lambda \\
\end{bmatrix} \right| = 0 \implies\\
	det \left |
	\begin{bmatrix}
	(4.4-\lambda) &  5.6 \\
	5.6  & (8-\lambda) \end{bmatrix} \right| = 0
$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(4/5)
- **Step 3.** Compute eigenvalues
	- `$(4.4-\lambda)\cdot (8-\lambda) - 5.6 \times 5.6 = 0 \\
	\implies \lambda^2 - 12.4 \lambda - 3.84 = 0\\
	\implies \lambda_1 = 0.32,\; \lambda_2 = 12.08(x = \frac{-b \pm \sqrt{b^2 -4ac}}{2a})$`
	- Compute eigenvector

	`$
	A\cdot \nu = \lambda_i\cdot \nu_i \implies
	\begin{bmatrix}
	4.4 &  5.6 \\
	5.6  & 8
	\end{bmatrix} \cdot
	\begin{bmatrix}
	x \\
	y
	\end{bmatrix} = 0.32\cdot
	\begin{bmatrix}
	x \\
	y
	\end{bmatrix} \\
	\begin{cases}
	4.4x + 5.6y = 0.32x\\
	5.6x + 8.0y = 0.32y
	\end{cases} \implies
	\nu_1 =
	\begin{bmatrix}
	x \\
	y
	\end{bmatrix} =
	\begin{bmatrix}
	-0.81 \\
	0.59
	\end{bmatrix}
	$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(5/5)
- **Step 3.** Compute eigenvector
	- `$
		\begin{cases}
		4.4x + 5.6y = 12.08x\\
		5.6x + 8.0y = 12.08y
		\end{cases} \implies
		\nu_2 =
		\begin{bmatrix}
		0.59\\
		0.81
		\end{bmatrix}
		$`
	- `$
		D = 
		\begin{bmatrix}
		-3 & -4\\
		-1 & -2\\
		-1 & 0\\
		1 & 0\\
		1 & 2\\
		3 & 4
		\end{bmatrix},
		V =
		\begin{bmatrix}
		\nu_1\\
		\nu_2
		\end{bmatrix} =  
		\begin{bmatrix}
		-0.81 & \underline{0.59} \\
		0.59 & \underline{0.81}
		\end{bmatrix}
		$`
- Order eigenvector `$\because \lambda_2 > \lambda_1, \therefore
	V =
	\begin{bmatrix}
	\underline{0.59} & -0.81 \\
	\underline{0.81} & 0.59 
	\end{bmatrix}	
	$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(6/6)
- **Step 4.** Create a feature vector `$D\cdot V$`
	- `$
		\begin{bmatrix}
		-3 & -4\\
		-1 & -2\\
		-1 & 0\\
		1 & 0\\
		1 & 2\\
		3 & 4
		\end{bmatrix}\cdot
		\begin{bmatrix}
		0.59 & -0.81 \\
		0.81 & 0.59 
		\end{bmatrix} =
		\begin{bmatrix}
		-5 & 0.1\\
		-2.2 & -0.4\\
		-0.6 & 0.8\\
		0.6 & -0.8\\
		2.2 & 0.4\\
		5 & -0.1
		\end{bmatrix}
		$`
	- `$ D\cdot V = \begin{bmatrix}
		pc1 & pc2
		\end{bmatrix}
		,\\
		pc1 = [-5, -2.2, -0.6, 0.8, 2.2, 5],\\ pc2 = [0.1, -0.4, 0.8, -0.8, 0.4, -0.1]
		$`
</script></section>
<section data-markdown><script type="text/template">
## An example of PCA(6/6)
- **Step 4.** Decide which principal components to keep.
	- `$Cov(pc1,pc1) = 12.08, \; Cov(pc2,pc2) = 0.32$`.
	- The pc1 store almost information and the pc2 include almost on information.
	We would like to delete the pc2 to reduce the dimensionality of data.
![](/images/dataMining/pca.png)
</script></section>
<section data-markdown><script type="text/template">
## A PCA's example with Breast Cancer Datasets
- [Performing Principal Component Analysis (PCA) with Cancer Dataset](https://medium.com/@jwbtmf/performing-principal-component-analysis-pca-with-cancer-dataset-ddbe5396b5b4)
- From above URL, PCA can be used to
	- Visualize multidimensional data.
	- Compress information.
	- Simplify complex business decisions.
	- Clarify convoluted(令人費解的) scientific processes.
- The example of PCA read breast cancer datasets from Scikit-learn. It **reduce 30 features to 2 features** and visualize with scatter and heatmap plot.
</script></section>
<section data-markdown><script type="text/template">
## Features Subset Selection
- Another way to **reduce dimensionality of data**
- **Redundant** features 
	- Duplicate much or all of the information contained in one or more other attributes
	- E.g., purchase price of a product and the amount of sales tax paid
- **Irrelevant** features
	- Contain no information that is useful for the data mining task at hand
	- E.g., **students' ID** is often irrelevant to the task of **predicting students' GPA**
</script></section>
<section data-markdown><script type="text/template">
## Feature Creation
- **Create new attributes (features)** that can **capture the important information in a data set** more effectively than the original ones
- Three general methodologies
	- Attribute extraction
			- **Domain-specific**
	- **Mapping data to new space** (see: data reduction)
		- E.g., Fourier transformation, wavelet transformation, manifold approaches (not covered)
	- Attribute construction 
		- **Combining features** (see: discriminative frequent patterns in Chapter 7 of textbook)
		- **Data discretization**
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Numerosity Reduction
</script></section>
<section data-markdown><script type="text/template">
## Data Reduction
- **Reduce data volume by choosing alternative, smaller forms** of data representation Parametric methods (e.g., **regression**)
- Assume the data fits some model, estimate model parameters, store only the parameters, and discard the data (except possible outliers)
	- Ex.: Log-linear models—obtain value at a point in m-D space as the product on appropriate marginal subspaces 
	- Non-parametric methods 
- Do not assume models
	- Major families: **histograms, clustering, sampling**, ...
</script></section>
<section data-markdown><script type="text/template">
## Parametric Data Reduction
- **Linear regression**
	- Data modeled to fit a straight line
	- Often uses the least-square method to fit the line
- **Multiple regression**
	- Allows a response variable Y to be modeled as a linear function of multidimensional feature vector
- **Log-linear model**
	- Approximates discrete multidimensional probability distributions
- The [reference of my slides](./reference.html) have many open books for statistic that is very helpful for you.   
</script></section>
<section data-markdown><script type="text/template">
## Regression Analysis
- A collective name for techniques for the modeling and analysis of numerical data consisting of values of a **dependent variable (measurement)** and of **one or more independent variables (predictors)**
- The parameters are **estimated so as to give a "best fit"(最合適) of the data**
	- **Most commonly the best fit** is evaluated by using the **least squares method**, but other criteria(準則) have also been used
- Used for **prediction(預測)** (including forecasting(預報) of time-series data), inference(推理), hypothesis testing(假設檢定), and modeling of causal relationships(因果關係)
</script></section>
<section data-markdown><script type="text/template">
## Linear and Log-Linear Models
- Linear regression: **`$Y = a X + b$`**
	- Two regression coefficients, `$a$` and `$b$`, specify the line and are to be estimated by using the data at hand
	- Using the least squares criterion to the known values of `$Y_1, Y_2, \cdots, X_1, X_2, \cdots$`
- Multiple regression: **`$Y = b_0 + b_1\times X_1 + b_2\times X_2$`**
	- Many nonlinear functions can be transformed into the above
- Log-linear models:
	- Approximate discrete multidimensional probability distributions
	- **Dimensionality reduction and data smoothing**
</script></section>
<section data-markdown><script type="text/template">
## Histogram Analysis
- Divide data into buckets and store average (sum) for each bucket
<figure style="float: right; width: 50%">
<img src="/images/dataMining/hist.png" alt="histograms analysis">
</figure>
- Partitioning rules:
	- Equal-width: equal bucket range
	- Equal-frequency (or equal-depth)
</script></section>
<section data-markdown><script type="text/template">
## Clustering
- Partition data set into clusters **based on similarity**, and **store cluster representation** (e.g., centroid and diameter) only
- Can be very effective if data is clustered but not if data is “smeared(塗抹)”
- Can have hierarchical(階層) clustering and be stored in multi-dimensional index(索引) tree structures
- There are many choices of clustering definitions and clustering algorithms
- Cluster analysis will be studied in depth in Chapter 10 of textbook.
</script></section>
<section data-markdown><script type="text/template">
## Sampling
- Sampling: **obtaining a small samples** to represent the whole data set N
- Allow a mining algorithm to run in complexity that is potentially sub-linear to the size of the data
- Key principle: Choose a representative subset of the data
	- Simple random sampling may have very poor performance in the presence of skew(歪斜)
	- Develop adaptive sampling methods, e.g., stratified sampling: 
- Sampling **may not reduce database I/Os** (page at a time)
</script></section>
<section data-markdown><script type="text/template">
## Types of Sampling
- Simple **random** sampling
	- There is an equal probability of selecting any particular item
- Sampling **without replacement**
	- Once an object is selected, it is removed from the population
- Sampling **with replacement**
	- A selected object is not removed from the population
- Stratified(分層) sampling: 
	- Partition the data set, and draw samples from each partition (proportionally, i.e., approximately the same percentage of the data) 
	- Used in conjunction with skewed data
</script></section>
<section data-markdown><script type="text/template">
## Data Cube Aggregation
- The **lowest level of a data cube** (base cuboid)
	- The aggregated(聚合) data for an individual entity of interest
	- E.g., **a customer in a phone calling data warehouse**
- **Multiple levels** of aggregation in data cubes
	- Further reduce the size of data to deal with
- Reference appropriate levels
	- Use the **smallest representation** which is enough to solve the task
- Queries regarding aggregated information should be answered using data cube, when possible
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
## Data Compression
- **String** compression
	- There are extensive theories and well-tuned algorithms
	- Typically lossless, but only limited manipulation is possible without expansion
- **Audio/video** compression
	- Typically lossy compression, with progressive refinement
	- Sometimes small fragments of signal can be reconstructed without reconstructing the whole
- **Time sequence is not audio**
	- Typically **short** and vary **slowly** with time
- **Dimensionality** and **numerosity reduction** may also be considered as forms of data compression
</script></section>
</section><!--end block-->

<section><!--start block-->
<section data-markdown><script type="text/template">
# Data Transformation
</script></section>
<section data-markdown><script type="text/template">
## What is Data Transformation
- A function that **maps the entire set of values** of a given attribute **to a new set of replacement values** s.t. each old value can be identified with one of the new values
- Methods
	- **Smoothing: Remove noise** from data
	- **Attribute/feature construction**
		- New attributes constructed from the given ones
	- **Aggregation**: Summarization, data cube construction
	- **Normalization**: Scaled to fall within a smaller, specified range:
	min-max normalization, z-score normalization, normalization by decimal scaling
- **Discretization**(離散化): Concept hierarchy climbing
</script></section>
<section data-markdown><script type="text/template">
## Normalization
- [`$Min-Max$` normalization](./04data.html#/8/7) : `$x_i$` are values of feature.
`$$
 x^{'}_{i} = \frac{x_i - x_{min}}{x_{max} - x_{min}} \in [0, 1]
$$`
- **Z-score normalization**: `$x$` is score, `$\mu$` is mean, `$\sigma$` is standard deviation.
`$$
 Z = \frac{x - \mu}{x_{\sigma}}
$$`
- Normalization by decimal scaling: `$j$` is the smallest integer such that `$Max(|D|) < 1\; ,$`
`$
	D_i = \frac{x_i}{10^j}
$`
</script></section>
<section data-markdown><script type="text/template">
## Discretization
- Three types of attributes
	- Nominal—values from an unordered set, e.g., color
	- Ordinal—values from an ordered set, e.g. ranking 
	- Numeric—real numbers, e.g., integer or real numbers
- Discretization: **Divide the range of a continuous attribute into intervals(間隔)**.
Interval labels can then be used to replace actual data values 
	- Reduce data size by discretization
	- **Supervised(監督) vs. unsupervised**
	- **Split (top-down) vs. merge (bottom-up)**
	- Discretization can be performed recursively on an attribute
	- Prepare for further analysis, e.g., **classification**(分類)
</script></section>
<section data-markdown><script type="text/template">
## Data Discretization Methods
- Typical methods: All the methods can be applied recursively
	- **Binning(分級)**
		- Top-down split, unsupervised
	- **Histogram(直方圖) analysis**
		- Top-down split, unsupervised
	- **Clustering(分群) analysis** (unsupervised, top-down split or bottom-up merge)
	- **Decision-tree analysis** (supervised, top-down split)
	- **Correlation(相關) analysis** (unsupervised, bottom-up merge) e.g., `$\chi^2$`(卡方檢定) 
</script></section>
</section><!--end block-->

</div>
</div>

<script src="/revealjs/lib/js/head.min.js"></script>
<script src="/revealjs/dist/reveal.js"></script>
<script src="/revealjs/plugin/zoom/zoom.js"></script>
<script src="/revealjs/plugin/notes/notes.js"></script>
<script src="/revealjs/plugin/search/search.js"></script>
<script src="/revealjs/plugin/markdown/markdown.js"></script>
<script src="/revealjs/plugin/highlight/highlight.js"></script>
<script src="/revealjs/plugin/math/math.js"></script>
<script src="/revealjs/plugin/menu/menu.js"></script>
<script src="/revealjs/plugin/chalkboard/plugin.js"></script>
<script src="/revealjs/plugin/customcontrols/plugin.js"></script>
<script src="/revealjs/plugin/animate/svg.min.js"></script>
<script src="/revealjs/plugin/animate/plugin.js"></script>

<script>
Reveal.initialize({
controls: true,
progress: true,
history: true,
center: true,
slideNumber: true,
mouseWheel: true,
transition: 'slide', // none/fade/slide/convex/concave/zoom

menu: {
	side: 'left',
	width: 'normal',
	numbers: false,
	titleSelector: 'h1, h2, h3, h4, h5, h6',
	useTextContentForMissingTitles: false,
	hideMissingTitles: false,
	markers: true,
	custom: true,
	themes: true,
	themesPath: '/revealjs/dist/theme/',
	transitions: true,
	openButton: true,
	openSlideNumber: false,
	keyboard: true,
	sticky: false,
	autoOpen: true,
	delayInit: false,
	openOnInit: false,
	loadIcons: true,
	
	custom: [
			{ title: 'TOC', icon: '<i class="fa fa-external-link-alt">', src: 'links.html' },
			{ title: 'About', icon: '<i class="fa fa-info">', content: '<p>Slides for teaching Office Suite Softwar</p>' }
	]
},

customcontrols: {
	controls: [
	{
		id: 'toggle-overview',
		title: 'Toggle overview (O)',
		icon: '<i class="fa fa-th"></i>',
		action: 'Reveal.toggleOverview();'
	},
	{ icon: '<i class="fa fa-pen-square"></i>',
		title: 'Toggle chalkboard (B)',
		action: 'RevealChalkboard.toggleChalkboard();'
	},
	{ icon: '<i class="fa fa-pen"></i>',
		title: 'Toggle notes canvas (C)',
		action: 'RevealChalkboard.toggleNotesCanvas();'
	}
	]
},

toolbar: {
	// Specifies where the toolbar will be shown: 'top' or 'bottom'
	position: 'bottom',

	// Add button to toggle fullscreen mode for the presentation
	fullscreen: true,

	// Add button to toggle the overview mode on and off
	overview: true,

	// Add button to pause (hide) the presentation display
	pause: true,

	// Add button to show the speaker notes
	notes: false,

	// Add button to show the help overlay
	help: false,

	// If true, the reveal.js-menu will be moved into the toolbar.
	// Set to false to leave the menu on its own.
	captureMenu: true,

	// If true, the playback control will be moved into the toolbar.
	// This is only relevant if the presentation is configured to autoSlide.
	// Set to false to leave the menu on its own.
	capturePlaybackControl: true,

	// By default the menu will load it's own font-awesome library
	// icons. If your presentation needs to load a different
	// font-awesome library the 'loadIcons' option can be set to false
	// and the menu will not attempt to load the font-awesome library.
	loadIcons: true
},

// Optional reveal.js plugins
plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight, RevealAnimate, RevealMenu, RevealCustomControls, RevealChalkboard, RevealMath.KaTeX ],

dependencies: [
//{ src: '/revealjs/plugin/toolbar/toolbar.js' }
]
});

</script>

</body>
</html>
